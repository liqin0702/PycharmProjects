{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [2. 3. 4.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [2 3 4]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "tf.Tensor([1.5 2.5 3.5], shape=(3,), dtype=float32) \n",
      " tf.Tensor([6. 9.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#tf.reduce系列\n",
    "a = tf.constant([[1., 2., 3.], [2., 3., 4.,]], dtype=tf.float32)\n",
    "print(a)\n",
    "b = tf.cast(a, dtype=tf.int32)\n",
    "print(b)\n",
    "c = tf.reduce_min(a)\n",
    "d = tf.reduce_max(a)\n",
    "print(c, d)\n",
    "e = tf.reduce_mean(a, axis=0)\n",
    "f = tf.reduce_sum(a,axis=1)\n",
    "print(e,'\\n',f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32) \n",
      " tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[5. 5. 5.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[-3. -3. -3.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[16. 16. 16.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[64. 64. 64.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[2. 2. 2.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[6. 6. 6.]\n",
      " [6. 6. 6.]\n",
      " [6. 6. 6.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#四则运算及开方、次方和平方根\n",
    "a = tf.ones([1,3])\n",
    "b = tf.fill([1,3], 4.)\n",
    "print(a,'\\n',b)\n",
    "print(tf.add(a,b))\n",
    "print(tf.subtract(a,b))\n",
    "print(tf.multiply(a,b))\n",
    "print(tf.divide(b,a))\n",
    "print(tf.square(b))\n",
    "print(tf.pow(b,3))\n",
    "print(tf.sqrt(b))\n",
    "\n",
    "# 矩阵乘法\n",
    "c = tf.ones([3,2])\n",
    "d = tf.fill([2,3], 3.)\n",
    "print(tf.matmul(c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# 生成“输入特征/标签”对\n",
    "features = tf.constant([12, 23, 10, 17])\n",
    "labels = tf.constant([0, 1, 1, 0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter called\n",
      "hello python\n",
      "exit called\n",
      "exc_type :<class 'ZeroDivisionError'>\n",
      "exc_val :division by zero\n",
      "exc_tb :<traceback object at 0x7f086c078640>\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-528ca428ab26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mFoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello python\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# with上下文管理器\n",
    "class Foo():\n",
    "  def __enter__(self):\n",
    "    print(\"enter called\")\n",
    "  def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "    print(\"exit called\")\n",
    "    print(\"exc_type :%s\"%exc_type)\n",
    "    print(\"exc_val :%s\"%exc_val)\n",
    "    print(\"exc_tb :%s\"%exc_tb)\n",
    "\n",
    "\n",
    "with Foo() as foo:\n",
    "  print(\"hello python\")\n",
    "  a = 1/0\n",
    "  print(\"hello end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.GradientTape\n",
    "with tf.GradientTape() as tape:\n",
    "    w = tf.Variable(tf.constant(3.))\n",
    "    loss = tf.pow(w, 2)\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.09003057 0.24472848 0.66524094], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softmax使参数缝合概率分布\n",
    "print(tf.nn.softmax(tf.constant([1., 2., 3.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999900000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.09003057+0.24472848+0.66524094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自更新\n",
    "w = tf.Variable(4)\n",
    "w.assign_sub(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [5 4 3]\n",
      " [8 7 2]], shape=(4, 3), dtype=int32)\n",
      "tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#tf.argmax找某一维度最大值的索引\n",
    "a = tf.constant([[1,2,3], [2,3,4], [5,4,3], [8,7,2]])\n",
    "print(a)\n",
    "print(tf.argmax(a,axis=0))\n",
    "print(tf.argmax(a,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data from datasets: \n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y_data from datasets: \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "x_data add index: \n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
      "0         5.1       3.5       1.4       0.2\n",
      "1         4.9       3.0       1.4       0.2\n",
      "2         4.7       3.2       1.3       0.2\n",
      "3         4.6       3.1       1.5       0.2\n",
      "4         5.0       3.6       1.4       0.2\n",
      "..        ...       ...       ...       ...\n",
      "145       6.7       3.0       5.2       2.3\n",
      "146       6.3       2.5       5.0       1.9\n",
      "147       6.5       3.0       5.2       2.0\n",
      "148       6.2       3.4       5.4       2.3\n",
      "149       5.9       3.0       5.1       1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "x_data add a column: \n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度  类别\n",
      "0         5.1       3.5       1.4       0.2     0\n",
      "1         4.9       3.0       1.4       0.2     0\n",
      "2         4.7       3.2       1.3       0.2     0\n",
      "3         4.6       3.1       1.5       0.2     0\n",
      "4         5.0       3.6       1.4       0.2     0\n",
      "..        ...       ...       ...       ...   ...\n",
      "145       6.7       3.0       5.2       2.3     2\n",
      "146       6.3       2.5       5.0       1.9     2\n",
      "147       6.5       3.0       5.2       2.0     2\n",
      "148       6.2       3.4       5.4       2.3     2\n",
      "149       5.9       3.0       5.1       1.8     2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#sklearn库和pandas库\n",
    "from sklearn import datasets\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "x_data = datasets.load_iris().data  # .data返回iris数据集所有输入特征\n",
    "y_data = datasets.load_iris().target  # .target返回iris数据集所有标签\n",
    "print(\"x_data from datasets: \\n\", x_data)\n",
    "print(\"y_data from datasets: \\n\", y_data)\n",
    "\n",
    "x_data = DataFrame(x_data, columns=['花萼长度', '花萼宽度', '花瓣长度', '花瓣宽度']) # 为表格增加行索引（左侧）和列标签（上方）\n",
    "pd.set_option('display.unicode.east_asian_width', True)  # 设置列名对齐\n",
    "print(\"x_data add index: \\n\", x_data)\n",
    "\n",
    "x_data['类别'] = y_data  # 新加一列，列标签为‘类别’，数据为y_data\n",
    "print(\"x_data add a column: \\n\", x_data)\n",
    "\n",
    "#类型维度不确定时，建议用print函数打印出来确认效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 11, 1111, 1]\n",
      "[3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "# numpy中的seed()和shuffle()函数\n",
    "import numpy as np\n",
    "x_data = [1, 11, 111, 1111]\n",
    "y_data = [1, 2, 3, 4]\n",
    "# 对应关系为1-1，,11-2，,111-3,1111-4\n",
    "np.random.seed(66) #括号里的为一个随机数，不重要\n",
    "np.random.shuffle(x_data) #让x_data乱序\n",
    "np.random.seed(66) #同样的随机数\n",
    "np.random.shuffle(y_data) # 在同一个随机数的作用下，y_data乱序的规则也一样\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-56c2714fe31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "a = [2,3,4,5]\n",
    "print(a.batch(2))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.2821310982108116\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 1, loss: 0.25459614396095276\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 2, loss: 0.22570250555872917\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 3, loss: 0.21028399839997292\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 4, loss: 0.19942265003919601\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 5, loss: 0.18873637542128563\n",
      "Test_acc: 0.5\n",
      "--------------------------\n",
      "Epoch 6, loss: 0.17851298674941063\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 7, loss: 0.16922875493764877\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 8, loss: 0.16107673197984695\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 9, loss: 0.15404684469103813\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 10, loss: 0.14802725985646248\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 11, loss: 0.14287303388118744\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 12, loss: 0.1384414117783308\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 13, loss: 0.13460607267916203\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 14, loss: 0.13126072101294994\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 15, loss: 0.12831821851432323\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 16, loss: 0.12570794485509396\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 17, loss: 0.12337299063801765\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 18, loss: 0.12126746773719788\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 19, loss: 0.11935433186590672\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 20, loss: 0.11760355159640312\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 21, loss: 0.11599068343639374\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 22, loss: 0.11449568346142769\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 23, loss: 0.11310207471251488\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 24, loss: 0.11179621331393719\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 25, loss: 0.11056671850383282\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 26, loss: 0.1094040758907795\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 27, loss: 0.10830028168857098\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 28, loss: 0.10724855028092861\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 29, loss: 0.10624313168227673\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 30, loss: 0.10527909733355045\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 31, loss: 0.10435222275555134\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 32, loss: 0.10345886088907719\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 33, loss: 0.1025958750396967\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 34, loss: 0.10176052898168564\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 35, loss: 0.10095042549073696\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 36, loss: 0.10016347467899323\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 37, loss: 0.09939785115420818\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 38, loss: 0.0986519306898117\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 39, loss: 0.09792428836226463\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 40, loss: 0.09721365198493004\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 41, loss: 0.09651889465749264\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 42, loss: 0.095839012414217\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 43, loss: 0.09517310559749603\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 44, loss: 0.09452036954462528\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 45, loss: 0.0938800759613514\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 46, loss: 0.09325155802071095\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 47, loss: 0.09263424575328827\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 48, loss: 0.09202759340405464\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 49, loss: 0.09143111854791641\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 50, loss: 0.09084436483681202\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 51, loss: 0.09026693925261497\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 52, loss: 0.08969846554100513\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 53, loss: 0.08913860842585564\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 54, loss: 0.08858705312013626\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 55, loss: 0.08804351650178432\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 56, loss: 0.08750772662460804\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 57, loss: 0.0869794450700283\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 58, loss: 0.08645843341946602\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 59, loss: 0.08594449236989021\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 60, loss: 0.08543741330504417\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 61, loss: 0.08493702113628387\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 62, loss: 0.08444313704967499\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 63, loss: 0.08395560085773468\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 64, loss: 0.08347426541149616\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 65, loss: 0.08299898356199265\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 66, loss: 0.08252961002290249\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 67, loss: 0.08206603676080704\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 68, loss: 0.08160812966525555\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 69, loss: 0.08115577884018421\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 70, loss: 0.08070887811481953\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 71, loss: 0.08026731014251709\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 72, loss: 0.07983098737895489\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 73, loss: 0.07939981110394001\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 74, loss: 0.0789736956357956\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 75, loss: 0.07855254411697388\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 76, loss: 0.078136270865798\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 77, loss: 0.07772480882704258\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 78, loss: 0.07731806114315987\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 79, loss: 0.07691597007215023\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 80, loss: 0.07651844993233681\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 81, loss: 0.07612543925642967\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 82, loss: 0.07573685422539711\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 83, loss: 0.07535265013575554\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 84, loss: 0.07497274503111839\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 85, loss: 0.07459708210080862\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 86, loss: 0.07422559149563313\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 87, loss: 0.0738582294434309\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 88, loss: 0.0734949205070734\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 89, loss: 0.0731356143951416\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 90, loss: 0.07278026826679707\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 91, loss: 0.07242879550904036\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 92, loss: 0.07208117935806513\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 93, loss: 0.0717373350635171\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 94, loss: 0.07139723561704159\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 95, loss: 0.07106081955134869\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 96, loss: 0.07072804030030966\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 97, loss: 0.07039883825927973\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 98, loss: 0.07007318455725908\n",
      "Test_acc: 0.8333333333333334\n",
      "--------------------------\n",
      "Epoch 99, loss: 0.06975101493299007\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 100, loss: 0.06943228747695684\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 101, loss: 0.06911696959286928\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 102, loss: 0.06880500074476004\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 103, loss: 0.06849635019898415\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 104, loss: 0.06819096114486456\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 105, loss: 0.06788879726082087\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 106, loss: 0.06758982129395008\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 107, loss: 0.0672939820215106\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 108, loss: 0.06700124498456717\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 109, loss: 0.06671156641095877\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 110, loss: 0.0664249137043953\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 111, loss: 0.06614123564213514\n",
      "Test_acc: 0.9\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112, loss: 0.06586050614714622\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 113, loss: 0.06558268330991268\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 114, loss: 0.06530773546546698\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 115, loss: 0.06503560114651918\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 116, loss: 0.06476627010852098\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 117, loss: 0.0644997013732791\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 118, loss: 0.06423585209995508\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 119, loss: 0.06397469434887171\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 120, loss: 0.06371619366109371\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 121, loss: 0.06346031092107296\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 122, loss: 0.06320701260119677\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 123, loss: 0.06295627076178789\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 124, loss: 0.06270804442465305\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 125, loss: 0.062462314032018185\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 126, loss: 0.06221904046833515\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 127, loss: 0.061978189274668694\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 128, loss: 0.06173973251134157\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 129, loss: 0.06150364316999912\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 130, loss: 0.06126988120377064\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 131, loss: 0.06103844102472067\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 132, loss: 0.06080926302820444\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 133, loss: 0.06058233417570591\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 134, loss: 0.06035762373358011\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 135, loss: 0.06013510935008526\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 136, loss: 0.05991474911570549\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 137, loss: 0.05969652906060219\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 138, loss: 0.059480417519807816\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 139, loss: 0.05926638934761286\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 140, loss: 0.059054410085082054\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 141, loss: 0.058844463899731636\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 142, loss: 0.058636522851884365\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 143, loss: 0.058430569246411324\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 144, loss: 0.058226557448506355\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 145, loss: 0.05802448280155659\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 146, loss: 0.05782430898398161\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 147, loss: 0.057626026682555676\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 148, loss: 0.05742959212511778\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 149, loss: 0.0572349950671196\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 150, loss: 0.05704221874475479\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 151, loss: 0.056851218454539776\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 152, loss: 0.05666198953986168\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 153, loss: 0.05647451616823673\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 154, loss: 0.05628875829279423\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 155, loss: 0.056104714050889015\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 156, loss: 0.05592233967036009\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 157, loss: 0.055741630494594574\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 158, loss: 0.05556255951523781\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 159, loss: 0.055385113693773746\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 160, loss: 0.05520926974713802\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 161, loss: 0.05503500811755657\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 162, loss: 0.054862307384610176\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 163, loss: 0.054691143333911896\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 164, loss: 0.05452151130884886\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 165, loss: 0.05435337871313095\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 166, loss: 0.05418673623353243\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 167, loss: 0.05402155593037605\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 168, loss: 0.053857834078371525\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 169, loss: 0.05369555111974478\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 170, loss: 0.0535346744582057\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 171, loss: 0.05337520223110914\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 172, loss: 0.053217110224068165\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 173, loss: 0.053060383535921574\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 174, loss: 0.05290501844137907\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 175, loss: 0.05275098141282797\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 176, loss: 0.052598257549107075\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 177, loss: 0.05244683753699064\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 178, loss: 0.05229670647531748\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 179, loss: 0.05214785132557154\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 180, loss: 0.052000245079398155\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 181, loss: 0.05185388308018446\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 182, loss: 0.05170875042676926\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 183, loss: 0.051564828492701054\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 184, loss: 0.05142210703343153\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 185, loss: 0.051280577667057514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 186, loss: 0.05114021338522434\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 187, loss: 0.05100100859999657\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 188, loss: 0.05086294375360012\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 189, loss: 0.05072600208222866\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 190, loss: 0.05059019569307566\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 191, loss: 0.050455485470592976\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 192, loss: 0.05032186862081289\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 193, loss: 0.05018934141844511\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 194, loss: 0.050057862885296345\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 195, loss: 0.04992745164781809\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 196, loss: 0.04979807883501053\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 197, loss: 0.04966974351555109\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 198, loss: 0.04954242613166571\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 199, loss: 0.049416118301451206\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 200, loss: 0.04929080791771412\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 201, loss: 0.04916647635400295\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 202, loss: 0.04904312081634998\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 203, loss: 0.048920733854174614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 204, loss: 0.04879929218441248\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 205, loss: 0.04867880139499903\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 206, loss: 0.048559242859482765\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 207, loss: 0.04844059329479933\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 208, loss: 0.04832287225872278\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 209, loss: 0.04820604436099529\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 210, loss: 0.04809010773897171\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 211, loss: 0.04797505680471659\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 212, loss: 0.04786087851971388\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 213, loss: 0.047747558914124966\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 214, loss: 0.0476350886747241\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 215, loss: 0.04752346873283386\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 216, loss: 0.0474126823246479\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 217, loss: 0.047302715480327606\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 218, loss: 0.047193581238389015\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 219, loss: 0.047085246071219444\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 220, loss: 0.04697771091014147\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 221, loss: 0.04687096457928419\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 222, loss: 0.04676500242203474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 223, loss: 0.046659817919135094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 224, loss: 0.046555391512811184\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 225, loss: 0.04645172692835331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 226, loss: 0.046348823234438896\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 227, loss: 0.04624664504081011\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 228, loss: 0.04614521004259586\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 229, loss: 0.04604450147598982\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 230, loss: 0.0459445109590888\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 231, loss: 0.045845236629247665\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 232, loss: 0.045746659860014915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 233, loss: 0.0456487787887454\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 234, loss: 0.04555159900337458\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 235, loss: 0.0454550925642252\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 236, loss: 0.04535926412791014\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 237, loss: 0.0452641062438488\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 238, loss: 0.04516960587352514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 239, loss: 0.04507576208561659\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 240, loss: 0.044982570223510265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 241, loss: 0.04489002004265785\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 242, loss: 0.04479810409247875\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 243, loss: 0.044706812128424644\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 244, loss: 0.044616157189011574\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 245, loss: 0.04452610295265913\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 246, loss: 0.04443667083978653\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 247, loss: 0.044347843155264854\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 248, loss: 0.04425960686057806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 249, loss: 0.04417196847498417\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 250, loss: 0.044084908440709114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 251, loss: 0.04399843607097864\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 252, loss: 0.043912540189921856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 253, loss: 0.043827205896377563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 254, loss: 0.043742443434894085\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 255, loss: 0.043658241629600525\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 256, loss: 0.043574584648013115\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 257, loss: 0.0434914818033576\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 258, loss: 0.04340892285108566\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 259, loss: 0.04332689754664898\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 260, loss: 0.0432454077526927\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 261, loss: 0.04316444229334593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 262, loss: 0.04308399744331837\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 263, loss: 0.04300406388938427\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 264, loss: 0.04292464908212423\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 265, loss: 0.04284573905169964\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 266, loss: 0.04276733938604593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 267, loss: 0.042689429596066475\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 268, loss: 0.042612007819116116\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 269, loss: 0.042535084299743176\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 270, loss: 0.042458645068109035\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 271, loss: 0.042382679879665375\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 272, loss: 0.042307195253670216\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 273, loss: 0.04223217163234949\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 274, loss: 0.04215762298554182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 275, loss: 0.04208352975547314\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 276, loss: 0.04200989659875631\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 277, loss: 0.041936714202165604\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 278, loss: 0.04186398349702358\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 279, loss: 0.04179169703274965\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 280, loss: 0.041719851084053516\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 281, loss: 0.04164844751358032\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 282, loss: 0.04157747142016888\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 283, loss: 0.041506921872496605\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 284, loss: 0.04143680538982153\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 285, loss: 0.04136710148304701\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 286, loss: 0.04129781946539879\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 287, loss: 0.04122895281761885\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 288, loss: 0.04116049408912659\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 289, loss: 0.041092448867857456\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 290, loss: 0.04102479573339224\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 291, loss: 0.040957554243505\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 292, loss: 0.040890700183808804\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 293, loss: 0.04082423262298107\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 294, loss: 0.040758166462183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 295, loss: 0.04069248400628567\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 296, loss: 0.04062717594206333\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 297, loss: 0.040562245063483715\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 298, loss: 0.040497698821127415\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 299, loss: 0.04043352138251066\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 300, loss: 0.04036971274763346\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 301, loss: 0.0403062729164958\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 302, loss: 0.04024319164454937\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 303, loss: 0.04018046706914902\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 304, loss: 0.0401181080378592\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 305, loss: 0.040056094992905855\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 306, loss: 0.039994434453547\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 307, loss: 0.03993312316015363\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 308, loss: 0.039872157853096724\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 309, loss: 0.039811530616134405\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 310, loss: 0.03975124517455697\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 311, loss: 0.039691298734396696\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 312, loss: 0.03963167453184724\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 313, loss: 0.039572388399392366\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 314, loss: 0.03951342450454831\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 315, loss: 0.03945478983223438\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 316, loss: 0.03939648298546672\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 317, loss: 0.03933848673477769\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 318, loss: 0.03928081365302205\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 319, loss: 0.03922344697639346\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 320, loss: 0.039166401606053114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 321, loss: 0.03910966124385595\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 322, loss: 0.039053223095834255\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 323, loss: 0.03899709461256862\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 324, loss: 0.038941262755542994\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 325, loss: 0.03888573916628957\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 326, loss: 0.0388304959051311\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 327, loss: 0.03877556184306741\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 328, loss: 0.03872091509401798\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 329, loss: 0.0386665565893054\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 330, loss: 0.038612480740994215\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 331, loss: 0.03855870105326176\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 332, loss: 0.03850520076230168\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 333, loss: 0.0384519724175334\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 334, loss: 0.038399036042392254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 335, loss: 0.03834636928513646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 336, loss: 0.03829398099333048\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 337, loss: 0.03824185347184539\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 338, loss: 0.038189999759197235\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 339, loss: 0.038138418924063444\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 340, loss: 0.03808710351586342\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 341, loss: 0.03803604608401656\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 342, loss: 0.03798525454476476\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 343, loss: 0.037934715393930674\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 344, loss: 0.037884439807385206\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 345, loss: 0.03783441847190261\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 346, loss: 0.03778465138748288\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 347, loss: 0.03773513715714216\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 348, loss: 0.03768586413934827\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 349, loss: 0.037636840250343084\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 350, loss: 0.037588066421449184\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351, loss: 0.03753953380510211\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 352, loss: 0.03749124752357602\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 353, loss: 0.03744319686666131\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 354, loss: 0.03739538788795471\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 355, loss: 0.03734781965613365\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 356, loss: 0.03730047354474664\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 357, loss: 0.037253367248922586\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 358, loss: 0.037206494715064764\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 359, loss: 0.03715984430164099\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 360, loss: 0.03711343323811889\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 361, loss: 0.03706724150106311\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 362, loss: 0.037021270021796227\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 363, loss: 0.03697552578523755\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 364, loss: 0.036930006463080645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 365, loss: 0.03688469948247075\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 366, loss: 0.03683961136266589\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 367, loss: 0.03679474908858538\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 368, loss: 0.036750094033777714\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 369, loss: 0.03670564666390419\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 370, loss: 0.0366614181548357\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 371, loss: 0.036617396865040064\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 372, loss: 0.03657358791679144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 373, loss: 0.03652997827157378\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 374, loss: 0.03648658888414502\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 375, loss: 0.03644338669255376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 376, loss: 0.036400395911186934\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 377, loss: 0.036357597913593054\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 378, loss: 0.03631501505151391\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 379, loss: 0.036272619385272264\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 380, loss: 0.036230423487722874\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 381, loss: 0.03618842409923673\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 382, loss: 0.0361466147005558\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 383, loss: 0.036105004604905844\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 384, loss: 0.0360635737888515\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 385, loss: 0.036022352520376444\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 386, loss: 0.035981301218271255\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 387, loss: 0.03594044363126159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 388, loss: 0.035899773240089417\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 389, loss: 0.03585928678512573\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 390, loss: 0.03581898845732212\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 391, loss: 0.035778870806097984\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 392, loss: 0.03573893290013075\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 393, loss: 0.035699174739420414\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 394, loss: 0.03565959073603153\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 395, loss: 0.035620191134512424\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 396, loss: 0.035580961499363184\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 397, loss: 0.03554190881550312\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 398, loss: 0.035503033082932234\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 399, loss: 0.035464323591440916\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 400, loss: 0.035425792913883924\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 401, loss: 0.03538742754608393\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 402, loss: 0.0353492246940732\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 403, loss: 0.03531120577827096\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 404, loss: 0.03527334099635482\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 405, loss: 0.03523564524948597\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 406, loss: 0.03519811574369669\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 407, loss: 0.03516074130311608\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 408, loss: 0.03512354148551822\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 409, loss: 0.035086498130112886\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 410, loss: 0.03504961263388395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 411, loss: 0.03501288965344429\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 412, loss: 0.03497632220387459\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 413, loss: 0.03493991028517485\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 414, loss: 0.0349036636762321\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 415, loss: 0.03486755769699812\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 416, loss: 0.03483161563053727\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 417, loss: 0.0347958211787045\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 418, loss: 0.03476017527282238\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 419, loss: 0.034724689088761806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 420, loss: 0.03468934912234545\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 421, loss: 0.034654161892831326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 422, loss: 0.03461911762133241\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 423, loss: 0.034584223292768\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 424, loss: 0.03454947005957365\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 425, loss: 0.0345148635096848\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 426, loss: 0.03448040969669819\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 427, loss: 0.03444609045982361\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 428, loss: 0.034411918837577105\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 429, loss: 0.03437788691371679\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 430, loss: 0.0343439974822104\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 431, loss: 0.03431024681776762\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 432, loss: 0.034276628866791725\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 433, loss: 0.03424314875155687\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 434, loss: 0.03420981951057911\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 435, loss: 0.03417660994455218\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 436, loss: 0.0341435419395566\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 437, loss: 0.034110613632947206\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 438, loss: 0.03407781198620796\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 439, loss: 0.03404514491558075\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 440, loss: 0.03401260497048497\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 441, loss: 0.033980210311710835\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 442, loss: 0.03394793486222625\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 443, loss: 0.03391578374430537\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 444, loss: 0.03388377372175455\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 445, loss: 0.033851887565106153\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 446, loss: 0.033820133190602064\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 447, loss: 0.033788496162742376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 448, loss: 0.033756986260414124\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 449, loss: 0.03372561139985919\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 450, loss: 0.03369434690102935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 451, loss: 0.03366320999339223\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 452, loss: 0.03363220253959298\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 453, loss: 0.03360130963847041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 454, loss: 0.03357054013758898\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 455, loss: 0.03353989636525512\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 456, loss: 0.03350935876369476\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 457, loss: 0.03347895201295614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 458, loss: 0.03344865143299103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 459, loss: 0.03341847471892834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 460, loss: 0.03338842140510678\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 461, loss: 0.03335847705602646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 462, loss: 0.03332865331321955\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 463, loss: 0.033298940397799015\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 464, loss: 0.03326933644711971\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 465, loss: 0.03323985682800412\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 466, loss: 0.03321048431098461\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 467, loss: 0.03318122588098049\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 468, loss: 0.0331520726904273\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 469, loss: 0.03312302753329277\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 470, loss: 0.03309410251677036\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 471, loss: 0.03306529065594077\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 472, loss: 0.0330365770496428\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 473, loss: 0.03300797380506992\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 474, loss: 0.03297948185354471\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 475, loss: 0.03295108396559954\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 476, loss: 0.03292280435562134\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 477, loss: 0.032894630916416645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 478, loss: 0.03286655806005001\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 479, loss: 0.03283859323710203\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 480, loss: 0.0328107294626534\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 481, loss: 0.03278296673670411\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 482, loss: 0.03275530692189932\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 483, loss: 0.03272775374352932\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 484, loss: 0.032700295094400644\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 485, loss: 0.032672947738319635\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 486, loss: 0.03264568746089935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 487, loss: 0.03261852730065584\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 488, loss: 0.032591485884040594\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 489, loss: 0.032564534805715084\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 490, loss: 0.032537671737372875\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 491, loss: 0.03251090971753001\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 492, loss: 0.03248425014317036\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 493, loss: 0.03245767951011658\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 494, loss: 0.032431216444820166\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 495, loss: 0.032404834404587746\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 496, loss: 0.03237855713814497\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 497, loss: 0.03235237207263708\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 498, loss: 0.03232626663520932\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 499, loss: 0.032300274819135666\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
    "\n",
    "# 导入所需模块\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 导入数据，分别为输入特征和标签\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
    "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
    "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "\n",
    "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
    "# 用tf.Variable()标记参数可训练\n",
    "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
    "\n",
    "lr = 0.1  # 学习率为0.1\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
    "epoch = 500  # 循环500轮\n",
    "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
    "\n",
    "# 训练部分\n",
    "for epoch in range(epoch):  #数据集级别的循环，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  #batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
    "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
    "\n",
    "    # 测试部分\n",
    "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        # 使用更新后的参数进行预测\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
    "        # 将pred转换为y_test的数据类型\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        # 将每个batch的correct数加起来\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        # 将所有batch中的correct数加起来\n",
    "        total_correct += int(correct)\n",
    "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
    "        total_number += x_test.shape[0]\n",
    "    # 总的准确率等于total_correct/total_number\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc:\", acc)\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "plt.show()  # 画出图像\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
